{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning for NLP (Part 1, ICME Summer Workshop, 2019)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahsalo/TestRepo/blob/master/Deep_Learning_for_NLP_(Part_1%2C_ICME_Summer_Workshop%2C_2019).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3XJ0X7HHSo4",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for Natural Language Processing\n",
        "\n",
        "*Instructor: Luke de Oliveira*\n",
        "\n",
        "*Teaching Assistant: Alex Matton*\n",
        "\n",
        "*Date: August 16th, 2019*\n",
        "\n",
        "*Contact email: [lukedeo@ldo.io](mailto:lukedeo@ldo.io)*\n",
        "\n",
        "## Structure\n",
        "\n",
        "This notebook is split up into three parts. \n",
        "\n",
        "The first part is an introduction to \"wrangling\" text data in Python and how to prepare text data for use with Machine Learning algorithms. \n",
        "\n",
        "The second part walks through an implementation of a sentiment detection model with a Long Short-Term Memory network (LSTM). \n",
        "\n",
        "The third part will use a demo dataset to train a Semantic Chunking model for a conversational agent. \n",
        "\n",
        "To use a hardware accelerator (i.e., a GPU) navigate in the menu above to **`Runtime > Change runtime type > GPU`**.\n",
        "\n",
        "## License\n",
        "\n",
        "All code examples and code downloads are licensed under the (extremely permissive) [MIT license](https://opensource.org/licenses/MIT). My goal is to have this be a useful base for you, should you so desire.\n",
        "\n",
        "## Datasets\n",
        "\n",
        "This notebook will use two datasets: \n",
        "\n",
        "* A binary sentiment dataset, with reviews / produced content from Yelp, Amazon, and Twitter\n",
        "* A semantic chunking dataset for a virtual assistant use case, where we'll be understanding weather queries\n",
        "\n",
        "To download the sentiment dataset, run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4SxHIRSIyOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://ldo.io/icme-sws/2019/sentiment-data.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wz8Sev1I1qP",
        "colab_type": "text"
      },
      "source": [
        "To download the virtual assistant dataset, run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0vrZjG-I2jE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://ldo.io/icme-sws/2019/weather-assistant.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIdzT82cme1s",
        "colab_type": "text"
      },
      "source": [
        "To download the `icmenlp` package, we run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chc-IqyCme1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://ldo.io/icme-sws/2019/icmenlp.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vylb_7kNMhVO",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look in our VM's directory..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTc7CvhRMGUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7a7cqL2NGYA",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We'll be using Colaboratory built-in libraries (scikit-learn & Keras) in order to avoid set up!\n",
        "\n",
        "Let's set up our imports below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d0r_nXSme10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure if we change any imports, they're reflected in our notebook\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PNT-R4TM-jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTKdzjdXme15",
        "colab_type": "text"
      },
      "source": [
        "Now, we'll use our library for this tutorial - `icmenlp`. This library provides two main utilities -- first, a principled way to load the data for this session, and second, a vocabulary container, which we will describe later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb3rbWtNme16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import icmenlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Je6C42xMv_q",
        "colab_type": "text"
      },
      "source": [
        "Let's open our datasets using data loading functions that will provide us with a train-test-validate split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tnt_cRTOWU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ASSISTANT_DATA = 'weather-assistant.json'\n",
        "SENTIMENT_DATA = 'sentiment-data.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvoK8vprM75p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assistant_data = icmenlp.load_data(ASSISTANT_DATA, 'assistant')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9-SOpTIOC-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assistant_data['train'][4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLIlx7AGme2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a small utility to display chunked data a bit more easily\n",
        "def display_chunking(chunks):\n",
        "    sent = ''\n",
        "    for ch in chunks:\n",
        "        label = ch.get('label')\n",
        "        text = ch['text']\n",
        "        if not label:\n",
        "            sent += text\n",
        "        else:\n",
        "            sent += '[{} | {}]'.format(text, label.upper())\n",
        "    return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgzNOUhsme2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_chunking(assistant_data['train'][4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7lr0deyme2J",
        "colab_type": "text"
      },
      "source": [
        "# Manipulating Text Data for ML\n",
        "\n",
        "One of the most asked questions both from students and from industry concerns how to prepare text data for deep learning. Today, we're going to focus on **embeddings** (one of the more popular incarnations of this is Word2Vec). Right now, we'll learn how to prepare data for usage in a model that learns to embed text.\n",
        "\n",
        "The first step of any such pipeline is **tokenization**, that is, converting a single text or document into a *sequence* of *tokens*. For word level models, these tokens roughly correspond to words / contractions, and in a character model, this corresponds to individual bytes. Many modern methods use **subword** information, which allows you to make predictions over text that has words that were not trained on (the so-called OOV, or out of vocabulary, problem).\n",
        "\n",
        "For example, the sentence\n",
        "    \n",
        "    Is there a minimum balance I need to maintain in my accounts?\n",
        "    \n",
        "could be *tokenized* as:\n",
        "    \n",
        "    'Is', 'there', 'a', 'minimum', 'balance', 'I', 'need', 'to', 'maintain', 'in', 'my', 'accounts', '?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jm9Ej8Nme2K",
        "colab_type": "text"
      },
      "source": [
        "How can one systematically convert text into these *tokens* then? It turns out this is one of the most critical,  important, and underappreciated steps. It drastically differs from language to language, and requires a lot of care to ensure consistency. This is one of the reasons why **character level** or **subword** models can be so useful in applied settings with inconsistent spelling, grammar, and nomenclature.\n",
        "\n",
        "The dominant approach to doing word-based tokenization consists of using a [**regular expression**](https://en.wikipedia.org/wiki/Regular_expression). Regular expressions define a formal language for searching through strings for matches to a query. We'll use one here to work with our text in this tutorial "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcXtECQ_me2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re is the Python RegEx library\n",
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    return [\n",
        "        # Make sure there is no trailing whitespace\n",
        "        x.strip() \n",
        "        # Split the text on matches of at least one \"word\"\n",
        "        for x in re.split('(\\W+)', text) \n",
        "        # Only include the token if it is not null\n",
        "        if x.strip() \n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZAdMwC9me2M",
        "colab_type": "text"
      },
      "source": [
        "Let's load the sentiment dataset to try this out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcSdhtdOme2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_data = icmenlp.load_data(SENTIMENT_DATA, 'sentiment')\n",
        "\n",
        "# Let's grab out a random training point\n",
        "text = sentiment_data['train'][0][2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdGQHpaume2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2eNvo12me2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsyWN6VOme2U",
        "colab_type": "text"
      },
      "source": [
        "We will get into more detail about how embedding models work later on, but for now, let's discuss the conversion of text into a format that is useful for embedding models. Embedding models rely on an **integer** representation for each word, since we will use it as lookup into a **lookup table**.\n",
        "\n",
        "An important consideration when using deep learning models is the length of a piece of text, as well as a signal of the beginning and end of a sentence. Most deep learning models require that each batch of text passed in to the model have identical sentence lengths. We commonly solve this using **padding**, or adding a series of meaningless tokens to increase the length of a document. We can then use **masking** to ensure that our model does not incorporate these into the learning procedure.\n",
        "\n",
        "We'll use `<PAD>` as the pad token, and `<S>`/`</S>` to delimit the beginning of a sentence and the end of a sentence respectively. In addition, any tokens that are unknown to us (for example, a word that is in the test set but not the train set) are mapped to the `<UNK>` token.\n",
        "\n",
        "To map from tokens to integers, we will define a bijective (two-way) map from words to integers that we will learn from data.\n",
        "\n",
        "We're going to use the one from our `icmenlp` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TCctB5Tme2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's look inside the code of the utility provided\n",
        "icmenlp.VocabularyContainer??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5odW8dlZme2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a vocab collection object with the tokenizer we defined above\n",
        "vocab = icmenlp.VocabularyContainer(tokenizer=tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHBFEd37me2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = sentiment_data['train'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iav7-YqPme2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.fit(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a84iwuqrme2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.transform(['this product was bad!'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qsbzVxhme2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.transform(['this product was bad!', 'meh'], pad_length=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4qth8o-me2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.inverse_transform(vocab.transform(['this product was bad!']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdA5cAYeme2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.inverse_transform(vocab.transform(['this product was abhorrent!']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHdhbHaeme2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.inverse_transform(\n",
        "    vocab.transform(\n",
        "        ['this product was abhorrent!'], \n",
        "        pad_length=9, \n",
        "        add_start=True, \n",
        "        add_end=True\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEOxZIQme2p",
        "colab_type": "text"
      },
      "source": [
        "We now know how to preprocess text data for use in deep learning. To summarize:\n",
        "\n",
        "1.) Each document gets split into tokens, a process called tokenization.\n",
        "\n",
        "2.) A mapping is learned from the vocabulary to unique integers.\n",
        "\n",
        "3.) We have four special tokens -- the pad token `<PAD>`, the start-of-sentence token `<S>`, the end-of-sentence token `</S>`and the unknown token `<UNK>`.\n",
        "\n",
        "4.) We pad sentences with the `<PAD>` token to make them the same size.\n",
        "\n",
        "Now, let's learn about deep learning for NLP!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSUSN63Qme2p",
        "colab_type": "text"
      },
      "source": [
        "# End of Part 1!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELZW2IuF0DJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}