{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning for NLP (ICME Summer Workshop, 2019)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahsalo/TestRepo/blob/master/Deep_Learning_for_NLP_(ICME_Summer_Workshop%2C_2019).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3XJ0X7HHSo4",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for Natural Language Processing\n",
        "\n",
        "*Instructor: Luke de Oliveira*\n",
        "\n",
        "*Teaching Assistant: Alex Matton*\n",
        "\n",
        "*Date: August 16th, 2019*\n",
        "\n",
        "*Contact email: [lukedeo@ldo.io](mailto:lukedeo@ldo.io)*\n",
        "\n",
        "## Structure\n",
        "\n",
        "This notebook is split up into three parts. \n",
        "\n",
        "The first part is an introduction to \"wrangling\" text data in Python and how to prepare text data for use with Machine Learning algorithms. \n",
        "\n",
        "The second part walks through an implementation of a sentiment detection model with a Long Short-Term Memory network (LSTM). \n",
        "\n",
        "The third part will use a demo dataset to train a Semantic Chunking model for a conversational agent. \n",
        "\n",
        "To use a hardware accelerator (i.e., a GPU) navigate in the menu above to **`Runtime > Change runtime type > GPU`**.\n",
        "\n",
        "## License\n",
        "\n",
        "All code examples and code downloads are licensed under the (extremely permissive) [MIT license](https://opensource.org/licenses/MIT). My goal is to have this be a useful base for you, should you so desire.\n",
        "\n",
        "## Datasets\n",
        "\n",
        "This notebook will use two datasets: \n",
        "\n",
        "* A binary sentiment dataset, with reviews / produced content from Yelp, Amazon, and Twitter\n",
        "* A semantic chunking dataset for a virtual assistant use case, where we'll be understanding weather queries\n",
        "\n",
        "To download the sentiment dataset, run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4SxHIRSIyOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://ldo.io/icme-sws/2019/sentiment-data.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wz8Sev1I1qP",
        "colab_type": "text"
      },
      "source": [
        "To download the virtual assistant dataset, run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0vrZjG-I2jE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://ldo.io/icme-sws/2019/weather-assistant.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIdzT82cme1s",
        "colab_type": "text"
      },
      "source": [
        "To download the `icmenlp` package, we run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chc-IqyCme1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://ldo.io/icme-sws/2019/icmenlp.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vylb_7kNMhVO",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look in our VM's directory..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTc7CvhRMGUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7a7cqL2NGYA",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We'll be using Colaboratory built-in libraries (scikit-learn & Keras) in order to avoid set up!\n",
        "\n",
        "Let's set up our imports below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d0r_nXSme10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure if we change any imports, they're reflected in our notebook\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PNT-R4TM-jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTKdzjdXme15",
        "colab_type": "text"
      },
      "source": [
        "Now, we'll use our library for this tutorial - `icmenlp`. This library provides two main utilities -- first, a principled way to load the data for this session, and second, a vocabulary container, which we will describe later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb3rbWtNme16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import icmenlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Je6C42xMv_q",
        "colab_type": "text"
      },
      "source": [
        "Let's open our datasets using data loading functions that will provide us with a train-test-validate split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tnt_cRTOWU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ASSISTANT_DATA = 'weather-assistant.json'\n",
        "SENTIMENT_DATA = 'sentiment-data.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvoK8vprM75p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assistant_data = icmenlp.load_data(ASSISTANT_DATA, 'assistant')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9-SOpTIOC-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assistant_data['train'][4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLIlx7AGme2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a small utility to display chunked data a bit more easily\n",
        "def display_chunking(chunks):\n",
        "    sent = ''\n",
        "    for ch in chunks:\n",
        "        label = ch.get('label')\n",
        "        text = ch['text']\n",
        "        if not label:\n",
        "            sent += text\n",
        "        else:\n",
        "            sent += '[{} | {}]'.format(text, label.upper())\n",
        "    return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgzNOUhsme2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_chunking(assistant_data['train'][4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7lr0deyme2J",
        "colab_type": "text"
      },
      "source": [
        "# Manipulating Text Data for ML\n",
        "\n",
        "One of the most asked questions both from students and from industry concerns how to prepare text data for deep learning. Today, we're going to focus on **embeddings** (one of the more popular incarnations of this is Word2Vec). Right now, we'll learn how to prepare data for usage in a model that learns to embed text.\n",
        "\n",
        "The first step of any such pipeline is **tokenization**, that is, converting a single text or document into a *sequence* of *tokens*. For word level models, these tokens roughly correspond to words / contractions, and in a character model, this corresponds to individual bytes. Many modern methods use **subword** information, which allows you to make predictions over text that has words that were not trained on (the so-called OOV, or out of vocabulary, problem).\n",
        "\n",
        "For example, the sentence\n",
        "    \n",
        "    Is there a minimum balance I need to maintain in my accounts?\n",
        "    \n",
        "could be *tokenized* as:\n",
        "    \n",
        "    'Is', 'there', 'a', 'minimum', 'balance', 'I', 'need', 'to', 'maintain', 'in', 'my', 'accounts', '?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jm9Ej8Nme2K",
        "colab_type": "text"
      },
      "source": [
        "How can one systematically convert text into these *tokens* then? It turns out this is one of the most critical,  important, and underappreciated steps. It drastically differs from language to language, and requires a lot of care to ensure consistency. This is one of the reasons why **character level** or **subword** models can be so useful in applied settings with inconsistent spelling, grammar, and nomenclature.\n",
        "\n",
        "The dominant approach to doing word-based tokenization consists of using a [**regular expression**](https://en.wikipedia.org/wiki/Regular_expression). Regular expressions define a formal language for searching through strings for matches to a query. We'll use one here to work with our text in this tutorial "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcXtECQ_me2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re is the Python RegEx library\n",
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    return [\n",
        "        # Make sure there is no trailing whitespace\n",
        "        x.strip() \n",
        "        # Split the text on matches of at least one \"word\"\n",
        "        for x in re.split('(\\W+)', text) \n",
        "        # Only include the token if it is not null\n",
        "        if x.strip() \n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZAdMwC9me2M",
        "colab_type": "text"
      },
      "source": [
        "Let's load the sentiment dataset to try this out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcSdhtdOme2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_data = icmenlp.load_data(SENTIMENT_DATA, 'sentiment')\n",
        "\n",
        "# Let's grab out a random training point\n",
        "text = sentiment_data['train'][0][2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdGQHpaume2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2eNvo12me2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsyWN6VOme2U",
        "colab_type": "text"
      },
      "source": [
        "We will get into more detail about how embedding models work later on, but for now, let's discuss the conversion of text into a format that is useful for embedding models. Embedding models rely on an **integer** representation for each word, since we will use it as lookup into a **lookup table**.\n",
        "\n",
        "An important consideration when using deep learning models is the length of a piece of text, as well as a signal of the beginning and end of a sentence. Most deep learning models require that each batch of text passed in to the model have identical sentence lengths. We commonly solve this using **padding**, or adding a series of meaningless tokens to increase the length of a document. We can then use **masking** to ensure that our model does not incorporate these into the learning procedure.\n",
        "\n",
        "We'll use `<PAD>` as the pad token, and `<S>`/`</S>` to delimit the beginning of a sentence and the end of a sentence respectively. In addition, any tokens that are unknown to us (for example, a word that is in the test set but not the train set) are mapped to the `<UNK>` token.\n",
        "\n",
        "To map from tokens to integers, we will define a bijective (two-way) map from words to integers that we will learn from data.\n",
        "\n",
        "We're going to use the one from our `icmenlp` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TCctB5Tme2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's look inside the code of the utility provided\n",
        "icmenlp.VocabularyContainer??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5odW8dlZme2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a vocab collection object with the tokenizer we defined above\n",
        "vocab = icmenlp.VocabularyContainer(tokenizer=tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHBFEd37me2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = sentiment_data['train'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iav7-YqPme2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.fit(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a84iwuqrme2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.transform(['this product was bad!'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qsbzVxhme2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.transform(['this product was bad!', 'meh'], pad_length=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4qth8o-me2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.inverse_transform(vocab.transform(['this product was bad!']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdA5cAYeme2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.inverse_transform(vocab.transform(['this product was abhorrent!']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHdhbHaeme2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.inverse_transform(\n",
        "    vocab.transform(\n",
        "        ['this product was abhorrent!'], \n",
        "        pad_length=9, \n",
        "        add_start=True, \n",
        "        add_end=True\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEOxZIQme2p",
        "colab_type": "text"
      },
      "source": [
        "We now know how to preprocess text data for use in deep learning. To summarize:\n",
        "\n",
        "1.) Each document gets split into tokens, a process called tokenization.\n",
        "\n",
        "2.) A mapping is learned from the vocabulary to unique integers.\n",
        "\n",
        "3.) We have four special tokens -- the pad token `<PAD>`, the start-of-sentence token `<S>`, the end-of-sentence token `</S>`and the unknown token `<UNK>`.\n",
        "\n",
        "4.) We pad sentences with the `<PAD>` token to make them the same size.\n",
        "\n",
        "Now, let's learn about deep learning for NLP!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSUSN63Qme2p",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "For the next segment, we're going to train a sentiment prediction model using deep learning. In particular, we're going to use a recurrent neural network (RNN) to process text **token by token**. We'll be using words as tokens.\n",
        "\n",
        "An RNN uses a **cell** to process each timestep of s sequence. In this case, the cell of our RNN will process text one word at a time. A common problem with RNNs is that early timesteps are forgotten, and our network has no signal related to early timesteps. A Long short-term memory (LSTM) network solves this by introducing a better method for retaining state/memory via a **memory cell**, and introducing a **forget gate**, which allows the network to learn when to forget.\n",
        "\n",
        "Let's walk through how to build a model for our sentiment analysis task. \n",
        "\n",
        "*Best-practice tip*: we're going to create and use a **new class** to make sure we can keep track of the preprocessing that goes into such a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym6kxncPme2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "class BinarySentimentModel():\n",
        "\n",
        "    def __init__(self, embedding_dim=128, lstm_size=256, bidirectional=False,\n",
        "                 optimizer='adam'):\n",
        "        \"\"\"Create a new model to handle a binary sentiment task.\n",
        "\n",
        "        The wrapper class will hold all state with respect to preprocessing,\n",
        "        transformation, and model training.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim (int): The dimension of the \"word vectors\" to be\n",
        "                trained in the model.\n",
        "            lstm_size (int): The number of hidden units in the LSTM.\n",
        "            bidirectional (bool): Whether or not to process the data using a\n",
        "                bidirectional or single-directional LSTM.\n",
        "            optimizer (str | keras.optimizers.Optimizer): The optimizer to use\n",
        "                when optimizing the loss on the training set using Stochastic\n",
        "                Gradient Descent (SGD).\n",
        "        \"\"\"\n",
        "        # We need to convert the names of labels to integers for our model\n",
        "        self.labelencoder = LabelEncoder()\n",
        "\n",
        "        # We're going to use our vocab container from before to store all the\n",
        "        # token -> ID mappings\n",
        "        self.vocab = icmenlp.VocabularyContainer(tokenizer=tokenize)\n",
        "        self.model = None\n",
        "\n",
        "        # We're storing *hyperparameters* of the model here\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lstm_size = lstm_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def make_model(self, vocab_size):\n",
        "        \"\"\"Creates a new keras model for the class.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The number of unique tokens contained in the\n",
        "                training vocabulary\n",
        "\n",
        "        Returns:\n",
        "            keras.models.Model: A built and compiled Keras model for thre task.\n",
        "        \"\"\"\n",
        "        \n",
        "        # The None tells Keras that we can expect differing sentence lengths\n",
        "        text = keras.Input(shape=(None, ), dtype='int32')\n",
        "        \n",
        "        # TODO: Write model together!\n",
        "\n",
        "        output = None\n",
        "\n",
        "        model = keras.Model(text, output)\n",
        "\n",
        "        # We're going to use crossentropy as our loss function here\n",
        "        model.compile(optimizer=self.optimizer,\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['acc'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, documents, labels, validation_data=None, pad_length='max',\n",
        "            **kwargs):\n",
        "        \"\"\"Trains (or fits) the model on training data while validating\n",
        "        on validation data\n",
        "\n",
        "        Args:\n",
        "            documents (List[str]): A list of training documents\n",
        "            labels (List[str | int]): A list of training labels associated to\n",
        "                each document.\n",
        "            validation_data (Tuple[List[str], List[str]]): A tuple of\n",
        "                validation data of the form (val_documents, val_labels)\n",
        "            pad_length (str | int): The padding length to use for training.\n",
        "            **kwargs: Passed to keras.Model.fit\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If validation data is not of the correct format.\n",
        "        \"\"\"\n",
        "        # X will be an array of integers corresponding to words\n",
        "        X = np.array(\n",
        "            self.vocab.fit(documents).transform(\n",
        "                documents, pad_length=pad_length)\n",
        "        )\n",
        "\n",
        "        # y will be an array of integers corresponding to sentiment labels (0\n",
        "        # or 1)\n",
        "        y = self.labelencoder.fit_transform(labels)\n",
        "\n",
        "        if validation_data:\n",
        "            # Process validation data the same way we process our training data\n",
        "            if not len(validation_data) == 2:\n",
        "                raise ValueError('Validation data must be a tuple (X, y)')\n",
        "            documents_val, labels_val = validation_data\n",
        "            validation_data = (\n",
        "                np.array(self.vocab.transform(\n",
        "                    documents_val, pad_length=pad_length)),\n",
        "                self.labelencoder.transform(labels_val)\n",
        "            )\n",
        "            _ = kwargs.pop('validation_data', None)\n",
        "\n",
        "        # Construct our Keras model\n",
        "        self.model = self.make_model(vocab_size=self.vocab.vocab_size)\n",
        "\n",
        "        # In practice, we would set up a *callback* in order to stop\n",
        "        # training when the validation error is minimized\n",
        "        self.model.fit(X, y, validation_data=validation_data, **kwargs)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, documents, pad_length='max', **kwargs):\n",
        "        # To run a prediction, we have to run all the way from:\n",
        "        # text -> tokenization -> integers -> model\n",
        "        # Here, we only care to get the probabilities for each class\n",
        "        indices = np.array(self.vocab.transform(\n",
        "            documents, pad_length=pad_length))\n",
        "        return self.model.predict(indices, **kwargs).ravel()\n",
        "\n",
        "    def predict(self, documents, pad_length='max', **kwargs):\n",
        "        # We can use the above method to get probabilities per class, then\n",
        "        # we just take the most likely one and recover the label\n",
        "        label_inv = self.predict_proba(documents, pad_length=pad_length,\n",
        "                                       **kwargs)\n",
        "        return self.labelencoder.inverse_transform(1 * (label_inv > 0.5))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6go6u2x_me2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_model = BinarySentimentModel(bidirectional=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtfBVqIlAfvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text, labels = sentiment_data['train']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geBntsqVAheQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAh0dvT2me2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In a real application, we would use a EarlyStopping and ModelCheckpoint \n",
        "# callback to stop training at the bottom of the validation loss curve.\n",
        "sentiment_model.fit(\n",
        "    *sentiment_data['train'], \n",
        "    validation_data=sentiment_data['val'], \n",
        "    epochs=3\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vLeQEENme2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_model.predict([\n",
        "    'this was the worst product ive ever used!!!', \n",
        "    'pretty awesome product!!!'\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh0rIZzxme20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_model.predict_proba([\n",
        "    'this was the worst product ive ever used!!!', \n",
        "    'this was the best product ive ever used!!!'\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efhrOTkcme22",
        "colab_type": "text"
      },
      "source": [
        "# Virtual Assistant Dataset\n",
        "\n",
        "In this example, we're going to train a model that is able to peform query understanding on a dataset of weather requests to a virtual assistant. We're going to build an LSTM model to predict what recognized component of a query each word in a piece of dialog corresponds to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHbgsH9Gme22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's get an example to understand\n",
        "example = assistant_data['train'][60]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAed5m6Ime24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDHg_Njyme26",
        "colab_type": "text"
      },
      "source": [
        "How can entities be encoded into labels? There is much debate on this, but there are generally three approaches:\n",
        "\n",
        "* **IO-encoding**: Only encodes that a token is an entity of a given type or not\n",
        "* **BIO-encoding**: Encodes the begining of an entity with a `B`, and also any following tokens in an entity with an `I`\n",
        "* **BILUO-encoding**: Encodes the begining of an entity with a `B`, any following tokens in an entity with an `I`, and the last token of an entity with a `L`. Single token entities are a `U`\n",
        "\n",
        "For example:\n",
        "\n",
        "*How cold is it tomorrow evening?*\n",
        "\n",
        "IO:\n",
        "*How cold [`TEMP`] is it tomorrow [`TIME`] evening [`TIME`]?*\n",
        "\n",
        "BIO:\n",
        "*How cold [`B-TEMP`] is it tomorrow [`B-TIME`] evening [`I-TIME`]?*\n",
        "\n",
        "BILUO:\n",
        "*How cold [`U-TEMP`] is it tomorrow [`B-TIME`] evening [`L-TIME`]?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALEUL7yJme27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_labels(example, tokenizer=tokenize):\n",
        "    \"\"\"\n",
        "    We define an function that can take this data, and \n",
        "    return a sequence of tokens and a sequence of token-labels.\n",
        "    \"\"\"\n",
        "    tokenized_text = []\n",
        "    labels = []\n",
        "    for chunk in example:\n",
        "        tokenized_chunk_text = tokenize(chunk['text'])\n",
        "        if 'label' in chunk:\n",
        "            # We're doing a subobtimal thing jere\n",
        "            chunk_labels = [chunk['label'].upper()] * len(tokenized_chunk_text)\n",
        "        else:\n",
        "            chunk_labels = ['-'] * len(tokenized_chunk_text)\n",
        "        tokenized_text.extend(tokenized_chunk_text)\n",
        "        labels.extend(chunk_labels)\n",
        "    return tokenized_text, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftHkcuzVCmDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOPLeP2zme29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_word_labels(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-l7yjb6me2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class WeatherAssistantModel():\n",
        "\n",
        "    def __init__(self, embedding_dim=128, lstm_size=256, optimizer='adam'):\n",
        "        \"\"\"Create a new model to handle a weather assistant use case.\n",
        "\n",
        "        The wrapper class will hold all state with respect to preprocessing,\n",
        "        transformation, and model training.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim (int): The dimension of the \"word vectors\" to be\n",
        "                trained in the model.\n",
        "            lstm_size (int): The number of hidden units in the LSTM.\n",
        "            optimizer (str | keras.optimizers.Optimizer): The optimizer to use\n",
        "                when optimizing the loss on the training set using Stochastic\n",
        "                Gradient Descent (SGD).\n",
        "        \"\"\"\n",
        "        # We encode the label space and the vocabulary as `VocabularyContainer`\n",
        "        # for padding. Our text is pretokenized, so we don't need a tokenizer\n",
        "        self.labelencoder = icmenlp.VocabularyContainer(tokenizer=lambda x: x)\n",
        "        self.vocab = icmenlp.VocabularyContainer(tokenizer=lambda x: x)\n",
        "        self.model = None\n",
        "\n",
        "        # Store our hyperparameters\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lstm_size = lstm_size\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def make_model(self, nb_classes, vocab_size):\n",
        "        \"\"\"Creates a new keras model for the class.\n",
        "\n",
        "        Args:\n",
        "            nb_classes (int): The total number of entity classes to predict\n",
        "            vocab_size (int): The number of unique tokens contained in the\n",
        "                training vocabulary\n",
        "\n",
        "        Returns:\n",
        "            keras.models.Model: A built and compiled Keras model for the task.\n",
        "        \"\"\"\n",
        "\n",
        "        # The None tells Keras that we can expect differing sentence lengths\n",
        "        text = keras.Input(shape=(None, ), dtype='int32')\n",
        "        \n",
        "        # TODO: Write model together!\n",
        "\n",
        "        output = None\n",
        "\n",
        "        model = keras.Model(text, output)\n",
        "        model.compile(optimizer=self.optimizer,\n",
        "                      loss='sparse_categorical_crossentropy')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, chunked_documents, validation_data=None, **kwargs):\n",
        "        \"\"\"Trains (or fits) the model on training data while validating\n",
        "        on validation data\n",
        "\n",
        "        Args:\n",
        "            chunked_documents (List[List[Dict]]): Training data\n",
        "            validation_data (List[List[Dict]]): Validation data\n",
        "            **kwargs: Passed to keras.Model.fit\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # Get paired pre-tokenized documents with per-token tags\n",
        "        documents, labels = zip(*[\n",
        "            get_word_labels(example, tokenizer=tokenize)\n",
        "            for example in chunked_documents\n",
        "        ])\n",
        "\n",
        "        # We want our input documents and our output labels to be integers\n",
        "        X = np.array(self.vocab.fit(documents).transform(\n",
        "            documents, pad_length=30))\n",
        "        Y = np.expand_dims(\n",
        "            np.array(self.labelencoder.fit(\n",
        "                labels).transform(labels, pad_length=30)),\n",
        "            -1\n",
        "        )\n",
        "\n",
        "        nb_classes = len(np.unique(Y))\n",
        "        nb_classes = self.labelencoder.vocab_size\n",
        "\n",
        "        if validation_data:\n",
        "            # Process validation data identically to training data\n",
        "            documents_val, labels_val = zip(*[\n",
        "                get_word_labels(example, tokenizer=tokenize)\n",
        "                for example in validation_data\n",
        "            ])\n",
        "            validation_data = (\n",
        "                np.array(self.vocab.transform(documents_val, pad_length=30)),\n",
        "                np.expand_dims(np.array(self.labelencoder.transform(\n",
        "                    labels_val, pad_length=30)), -1)\n",
        "            )\n",
        "            _ = kwargs.pop('validation_data', None)\n",
        "\n",
        "        self.model = self.make_model(nb_classes=nb_classes,\n",
        "                                     vocab_size=self.vocab.vocab_size)\n",
        "\n",
        "        # In practice, we would set up a callback in order to stop\n",
        "        # training when the validation error is minimized\n",
        "        self.model.fit(X, Y, validation_data=validation_data, **kwargs)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, document):\n",
        "        # To run inference on a single document, we tokenize it, obtain indices\n",
        "        # run it though the model, then decode the most likely tags per token.\n",
        "        segments, _ = get_word_labels([{'text': document}], tokenizer=tokenize)\n",
        "        indices = np.array(self.vocab.transform(segments))\n",
        "        label_inv = self.model.predict(indices).argmax(-1).astype(int).tolist()\n",
        "        return list(zip(\n",
        "            segments, self.labelencoder.inverse_transform(label_inv)\n",
        "        ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8ViNITnme3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assistant_model = WeatherAssistantModel(lstm_size=256, embedding_dim=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ-WzXXfme3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In a real application, we would use a EarlyStopping and ModelCheckpoint \n",
        "# callback to stop training at the bottom of the validation loss curve.\n",
        "assistant_model.fit(assistant_data['train'], assistant_data['val'], \n",
        "                    epochs=20, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFh21quRme3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = assistant_model.predict(\"How cold is it tomorrow?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkzYYs_4me3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQD2uCScme3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "understood_entities = [\n",
        "    (word, ent_type[0])\n",
        "    for word, ent_type in result\n",
        "    if ent_type[0] != '-'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSCCPZWPme3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "understood_entities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AY0KoNrG-Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}